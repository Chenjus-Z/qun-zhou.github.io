---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
æˆ‘æ˜¯å±±ä¸œå¸ˆèŒƒå¤§å­¦çš„ä¸€åæ¨¡æ‹ŸICè®¾è®¡æ–¹å‘çš„ç ”äºŒå­¦ç”Ÿï¼ŒæŒ‡å¯¼è€å¸ˆæ˜¯ååº†([Qing Hua](http://www.physics.sdnu.edu.cn/info/1142/5730.htm))ã€‚ç ”ä¸€æ—¶è·Ÿéšå¯¼å¸ˆçš„æŒ‡å¯¼ï¼Œåšé«˜å‹é©±åŠ¨èŠ¯ç‰‡è®¾è®¡(1um)ï¼Œç†Ÿæ‚‰è®¾è®¡è½¯ä»¶å’Œè®¾è®¡æµç¨‹ã€‚åŒæ—¶è‡ªå­¦æ‹‰æ‰ç»´å’Œæ¡‘æ£®çš„ä¹¦ç±ï¼Œæ²‰æ·€æ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„åŸºæœ¬åŠŸåº•ã€‚ç ”äºŒï¼Œå¼€å§‹ç”±é«˜å‹é©±åŠ¨èŠ¯ç‰‡ç”µè·¯é‡Œçš„æ¨¡å—å¼•ç”³ï¼Œä¾‹å¦‚ç”µå¹³ä½ç§»å’Œç”µå‹åŸºå‡†æºç”µè·¯ï¼Œè¿›è¡Œå°æ¨¡å—ç”µè·¯åˆ›æ–°è®¾è®¡ã€‚é€šè¿‡å¯¹ç›¸å…³æ–¹å‘è®ºæ–‡çš„ç ”ç©¶å’Œæ€»ç»“ï¼Œä¸æ–­å°è¯•ï¼Œè‡ªä¸»è®¾è®¡äº†å¸¦æµ®åŠ¨è¡¬åº•çš„ç”µå¹³ä½ç§»ç”µè·¯(130nm)å’Œå¸¦å·®åˆ†å¼è¾“å‡ºçš„ç”µå‹åŸºå‡†æºç”µè·¯(180nm)ï¼Œå¦å¤–è¿˜è®¾è®¡äº†ä¸‰æ”¯è·¯é€»è¾‘æ§åˆ¶çš„ç”µå¹³ä½ç§»ç”µè·¯(55nm)ï¼Œæœ‰æœ›å‘è¡¨**ä¸‰ç¯‡è´¨é‡ä¸­ä¸Šçš„è®ºæ–‡**ã€‚æœ‰è¯»åšç»§ç»­æ·±é€ çš„æ‰“ç®—ï¼Œè¦åšå°±åšæœ€å¥½ã€‚ç›®å‰å¯¹ICç”µæºæ–¹å‘ï¼ˆäºšé˜ˆå€¼ä½åŠŸè€—ï¼‰è¿™ä¸€å—ç§¯ç´¯è¾ƒä¸ºæ·±åˆ»ï¼Œå¯¹å…¶å®ƒæ¨¡æ‹ŸICæ–¹å‘æŠ±æœ‰æœŸå¾…ã€‚

ä¸ªäººè‡ªè¯„ï¼šæœ‰ç›®æ ‡æœ‰è§„åˆ’ï¼Œè‡ªé©±åŠ›å¼ºï¼ŒæŠ—å‹èƒ½åŠ›å¼ºï¼Œèƒ½æœ‰æ•ˆè§£å†³ç ”ç©¶è¿‡ç¨‹ä¸­ç¢°åˆ°çš„é—®é¢˜ã€‚è‹±è¯­å£è¯­èƒ½åŠ›ä¸é”™ï¼Œèƒ½å’Œå¤–å›½äººæ— éšœç¢äº¤æµã€‚å­¦ä¹ èƒ½åŠ›å¼ºï¼Œç¡•å£«é˜¶æ®µå¾ˆå¤šç ”ç©¶æ–¹å‘éƒ½æ˜¯ç”±è‡ªå·±æŒ–æ˜ã€‚å¸Œæœ›èƒ½åœ¨æœªæ¥åšå¯¼çš„æŒ‡å¼•å’Œæ•™å¯¼ä¸‹ï¼Œåœ¨åšå£«æœŸé—´ï¼Œè‡³å°‘å‘è¡¨ä¸€ç¯‡é¡¶åˆŠã€‚å¦‚æœæœ‰åšå¯¼æ„Ÿå…´è¶£ï¼Œå¯ä»¥æ”¶è—æˆ‘çš„ç®€å†[CV](https://github.com/zhouchch3/changchunzhou/blob/main/docs/CV.pdf)

<!--My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>).-->

# ğŸ“– æ•™è‚²èƒŒæ™¯
- *2019.09 - 2023.06*, åœŸæœ¨å·¥ç¨‹, æˆéƒ½å¤§å­¦ï¼ˆè½¯ç§‘232ï¼‰.

  å¤§äºŒå†³å¿ƒå­¦ICè®¾è®¡ï¼Œè‡ªå­¦ç”µè·¯åŸºç¡€ã€æ¨¡æ‹Ÿç”µå­æŠ€æœ¯åŸºç¡€ã€æ•°å­—ç”µå­æŠ€æœ¯åŸºç¡€ã€ä¿¡å·ä¸ç³»ç»Ÿã€åŠå¯¼ä½“ç‰©ç†ç­‰å¤šé—¨è¯¾ç¨‹
- *2023.09 - è‡³ä»Š*, ç”µå­ç§‘å­¦ä¸æŠ€æœ¯, å±±ä¸œå¸ˆèŒƒå¤§å­¦ï¼ˆè½¯ç§‘105ï¼‰.

  è‡ªå­¦ã€Šæ¨¡æ‹ŸCMOSé›†æˆç”µè·¯è®¾è®¡ã€‹ã€ã€Šæ¨¡æ‹Ÿé›†æˆç”µè·¯è®¾è®¡ç²¾ç²¹ã€‹ç­‰ä¹¦ç±

<!--# ğŸ”¥ News
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. -->

# ğŸ”¥ æ¨¡æ‹ŸICè®¾è®¡ç»å†
- 2023.10~2024.6 ï¼š**600Vé«˜å‹é©±åŠ¨èŠ¯ç‰‡-1um**
  
  1ã€å‚ä¸ä¸ƒé€šé“é©±åŠ¨èŠ¯ç‰‡è®¾è®¡ï¼šåŒ…æ‹¬æ¬ å‹ä¿æŠ¤ç”µè·¯ï¼Œç”µå¹³ä½ç§»ç”µè·¯ï¼Œé«˜è¾¹é©±åŠ¨è„‰å†²ä¿¡å·äº§ç”Ÿç”µè·¯ç­‰ã€‚
  
  2ã€ä¸¤é€šé“é©±åŠ¨èŠ¯ç‰‡ï¼šæ¯ä¸ªæ¨¡å—çš„ç”µè·¯è®¾è®¡å’Œç‰ˆå›¾è®¾è®¡ã€‚ç”µè·¯èƒ½å°†5Vçš„è¾“å…¥ä¿¡å·é©±åŠ¨600Vé«˜è¾¹è¾“å‡ºå’Œ15Vä½è¾¹è¾“å‡ºã€‚æœ€ç»ˆåŠæ¡¥å¼é©±åŠ¨åŠŸç‡å™¨ä»¶ã€‚å¯¹ç”µå‹åŸºå‡†å’Œç”µå¹³ä½ç§»ç­‰éƒ¨åˆ†è¿›è¡Œä¼˜åŒ–ï¼Œè¾¾åˆ°äº†ä½å»¶æ—¶çš„ç”µè·¯ç‰¹æ€§ã€‚
- 2024.04~2024.8ï¼š**å¸¦æµ®åŠ¨è¡¬åº•çš„ç”µå¹³ä½ç§»ç”µè·¯-130nm**
  
  1ã€æå‡ºäº†ä¸€ç§æµ®åŠ¨è¡¬åº•æŠ€æœ¯ï¼Œæ ¹æ®ç”µè·¯è¾“å‡ºé€»è¾‘ï¼Œåé¦ˆæä¾›å»¶æ—¶çš„å¤§å°å¯æ§çš„æµ®åŠ¨ç”µå‹ï¼Œç»™ç”µè·¯è¾“å…¥ç›´æ¥é©±åŠ¨çš„æ­£åé€»è¾‘å™¨ä»¶æä¾›è¡¬åº•åç½®ã€‚å‘¨æœŸæ€§åœ°é™ä½å™¨ä»¶çš„é˜ˆå€¼ç”µå‹ï¼ŒåŒæ—¶é™ä½ç”µå¹³ä½ç§»æ—¶ä¸Šä¸‹æ²¿çš„å»¶è¿Ÿæ—¶é—´ï¼Œå°äº2nSï¼Œæœ€å°è¾“å…¥ç”µå‹é™è‡³0.17Vã€‚
  
  2ã€æå‡ºäº†ä¸€ç§ç”µå¹³ä½ç§»åŸºç¡€ç»“æ„ï¼Œå¸¦æœ‰åŠ¨æ€ç”µæµé™åˆ¶å™¨ä»¶ï¼ŒåŒæ—¶é™ä½äº†å¼€å…³çŠ¶æ€çš„é™æ€åŠŸè€—ã€‚æœ€ç»ˆEDPä¼˜äºåŒç±»å‹å¹³å‡æ°´å¹³ä¸€ä¸ªæ•°é‡çº§
- 2024.08~2024.12ï¼š**ä¸‰æ”¯è·¯é€»è¾‘æ§åˆ¶ç”µå¹³ä½ç§»-55nm**

  1ã€è®¾è®¡äº†ä¸‰æ”¯è·¯é€»è¾‘æ§åˆ¶çš„ç”µå¹³ä½ç§»ï¼Œå…·æœ‰éå¸¸ä½çš„é™æ€åŠŸè€—ï¼Œæ¯”åŒåˆ¶ç¨‹ç”µè·¯æœ‰æ›´å¿«çš„è½¬æ¢é€Ÿåº¦ã€‚
- 2024.04~2024.12ï¼š**CMOSç”µå‹åŸºå‡†æºè®¾è®¡-180nm**

  1ã€æå‡ºäº†ä¸Šåšä¸‹è–„çš„ä¼ªå…±æºå…±æ …ç”µæµé•œã€‚æä¾›çš„åç½®ç”µæµç²¾åº¦éå¸¸é«˜ï¼Œè¾“å‡ºç”µæµåå·®å æ¯”å°äº0.02%
  
  2ã€æå‡ºäº†ä¸¤çº§è‡ªåç½®çš„SDMTæ ¸å¿ƒã€‚æä¾›ä¸¤ä¸ªå‚è€ƒç”µå‹ã€‚ç”µå‹è‡ªåç½®é™ä½LSã€‚
  
  3ã€æå‡ºäº†ä¸²è”å·®åˆ†å¼è¾“å‡ºç»“æ„ã€‚å°†ä¸¤ä¸ªå‚è€ƒç”µå‹è¿›è¡Œå·®åˆ†ï¼ŒæŠµæ¶ˆPVTå¸¦æ¥çš„åå·®ï¼Œå¹¶ç»™äºˆPVTè¡¥å¿ã€‚é€šè¿‡å·®åˆ†å’Œè¡¥å¿ï¼ŒLSé™ä½ä¸€åŠï¼ŒPSRRå…¨é¢‘åŸŸè‡³å°‘ä¸‹é™6dBï¼Œå·¥è‰ºè§’åå·®å°äº10mVã€‚æ¸©åº¦åå·®ä¸‹é™è‡³1mVä»¥å†…ã€‚
  
  4ã€æå‡ºå…³é”®æç‚¹åç§»æŠ€æœ¯ã€‚PSRRæå€¼ä¸‹é™äº†20%ã€‚é€šè¿‡æç‚¹å åŠ åŠ é€ŸPSRRè¡°å‡ï¼Œé«˜é¢‘PSRRä¸‹é™äº†7dBã€‚
  
  5ã€æå‡ºäº†åŒè¾“å…¥å¯åŠ¨ç”µè·¯ã€‚ä¸å•è¾“å…¥å¯åŠ¨ç”µè·¯ç›¸æ¯”ï¼Œé™æ€åŠŸè€—ä¸‹é™äº†ä¸‰ä¸ªæ•°é‡çº§ã€‚

# ğŸ“ æˆæœè½¬æ¢

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCAS-II 2025ï¼ˆåœ¨æŠ•ï¼‰</div><img src='images/å›¾ä¸‰.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


**<u>Qun Zhou</u>**, Kang Zeng, Weiwei Yue and Qing Hua, Member, IEEE

*IEEE Transactions on Circuits and Systems II: Express Briefs (**TCAS-II**)*

<details>
<summary>Abstract</summary>
This brief presents an ultra-fast voltage level shifter (VLS) with a wide voltage conversion range. The proposed VLS achieves low symmetric propagation delay while operating in the subthreshold voltage, utilizing a newly introduced floating bias technique (FBT). By integrating a novel dynamic current limiter structure with a split inverter and pull-down devices, both the low-to-high and high-to-low delays are largely reduced. The  design of the proposed VLS is divided into two sections: the basic version and the enhanced version, which incorporates the FBT. Using a standard 130nm CMOS technology, the basic version demonstrates low propagation delay and high energy efficiency. Post-layout simulations, taking into account process, voltage, and temperature (PVT) variations, have been performed. With the implementation of the FBT, the delay is reduced to 2.09ns, dynamic power consumption is lowered to 27.3fJ, the minimum supply voltage is 0.17V, and the static power consumption is 9.7nW for the enhanced version.

</details>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCAS-I 2025ï¼ˆåœ¨æµ‹ï¼‰</div><img src='images/TCAS-II.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Adjustable Multi-Stream Block-Wise Farthest Point Sampling Acceleration in Point Cloud Analysis](https://ieeexplore.ieee.org/document/10430381)

**<u>C. Zhou</u>**\*, Y. Fu*, Y. Ma, E. Han, Y. He, and H. Jiao

*IEEE Transactions on Circuits and Systems II: Express Briefs (**TCAS-II**)*


<details>
<summary>Abstract</summary>
Point cloud is increasingly used in a variety of applications. Farthest Point Sampling (FPS) is typically employed for down-sampling to reduce the size of point cloud and enhance the representational capability by preserving contour points in point cloud analysis. However, due to low parallelism and high computational complexity, high energy consumption and long latency are caused, which becomes a bottleneck of hardware acceleration. In this brief, we propose an adjustable multi-stream block-wise FPS, adjusted by four configurable parameters, according to hardware and accuracy requirements. A unified hardware architecture is designed to implement the adjustable multi-stream block-wise FPS. Furthermore, we present a rapid searching algorithm to select the optimal configuration of the four parameters. Designed in an industrial 28-nm CMOS technology, the proposed hardware architecture achieves a latency of 0.005 ms and a frame energy consumption of 0.09 ÂµJ/frame for 1 k input points at 200 MHz and 0.9 V supply voltage. Compared to the state of the art, the proposed hardware architecture reduces the latency by up to 84.38%, saves the energy by up to 76.19%, and improves the network accuracy by up to 1.05%.

</details>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCAD 2023</div><img src='images/ICCAD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Energy-Efficient 3D Point Cloud Neural Network Accelerator With Efficient Filter Pruning, MLP Fusion, and Dual-Stream Sampling](https://ieeexplore.ieee.org/document/10323704)

**<u>C. Zhou</u>**, Y. Fu, M. Liu, S. Qiu, G. Li, Y. He, and H. Jiao.

*IEEE/ACM International Conference On Computer Aided Design (**ICCAD**)*


<details>
<summary>Abstract</summary>
Three-dimensional (3D) point cloud has been employed in a wide range of applications recently. As a powerful weapon for point cloud analysis, point-based point cloud neural networks (PNNs) have demonstrated superior performance with less computation complexity and parameters, compared to sparse 3D convolution-based networks and graph-based convolutional neural networks. However, point-based PNNs still suffer from high computational redundancy, large off-chip memory access, and low parallelism in hardware implementation, thereby hindering the applications on edge devices. In this paper, to address these challenges, an energy-efficient 3D point cloud neural network accelerator is proposed for on-chip edge computing. An efficient filter pruning scheme is used to skip the redundant convolution of pruned filters and zero-value feature channels. A block-wise multi-layer perceptron (MLP) fusion method is proposed to increase the on-chip reuse of features, thereby reducing off-chip memory access. A dual-stream blocking technique is proposed for higher parallelism while maintaining inference accuracy. Implemented in an industrial 28-nm CMOS technology, the proposed accelerator achieves an effective energy efficiency of 12.65 TOPS/W and 0.13 mJ/frame energy consumption for PointNeXt-S at 100 MHz, 0.9 V supply voltage, and 8-bit data width. Compared to the state-of-the-art point cloud neural network accelerators, the proposed accelerator enhances the energy efficiency by up to 66.6Ã— and reduces the energy consumption per frame by up to 70.2Ã—. 

</details>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IOT Journal 2023</div><img src='images/IOTJ.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Sagitta: An Energy-Efficient Sparse 3D-CNN Accelerator for Real-Time 3D Understanding.](https://ieeexplore.ieee.org/abstract/document/10224248/)

**<u>C. Zhou</u>**, M. Liu, S. Qiu, X. Cao, Y. Fu, Y. He, and H. Jiao.

*IEEE Internet of Things Journal (**IOT Journal**)*


<details>
<summary>Abstract</summary>
Three-dimensional (3D) understanding or inference has received increasing attention, where 3D convolutional neural networks (3D-CNNs) have demonstrated superior performance compared to two-dimensional CNNs (2D-CNNs), since 3D-CNNs learn features from all three dimensions. However, 3D-CNNs suffer from intensive computation and data movement. In this paper, Sagitta, an energy-efficient low-latency on-chip 3D-CNN accelerator, is proposed for edge devices. Locality and small differential value dropout are leveraged to increase the sparsity of activations. A full-zero-skipping convolutional microarchitecture is proposed to fully utilize the sparsity of weights and activations. A hierarchical load-balancing scheme is also introduced to increase the hardware utilization. Specialized architecture and computation flow are proposed to enhance the effectiveness of the proposed techniques. Fabricated in a 55-nm CMOS technology, Sagitta achieves 3.8 TOPS/W for C3D at a latency of 0.1 s and 4.5 TOPS/W for 3D U-Net at a latency of 0.9 s at 100 MHz and 0.91 V supply voltage. Compared to the state-of-the-art 3D-CNN and 2D-CNN accelerators, Sagitta enhances the energy efficiency by up to 379.6Ã— and 11Ã—, respectively.

</details>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">DAC 2021</div><img src='images/DAC.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Energy-Efficient Low-Latency 3D-CNN Accelerator Leveraging Temporal Locality, Full Zero-Skipping, and Hierarchical Load Balance](https://ieeexplore.ieee.org/document/9586299)

**<u>C. Zhou</u>**, M. Liu, S. Qiu, Y. He, and H. Jiao.

*IEEE/ACM Design Automation Conference (**DAC**)*


<details>
<summary>Abstract</summary>
Three-dimensional convolutional neural network (3D-CNN) has demonstrated outstanding classification performance in video recognition compared to two-dimensional CNN (2D-CNN), since 3D-CNN not only learns the spatial features of each frame, but also learns the temporal features across all frames. However, 3D-CNN suffers from intensive computation and data movement. To solve these issues, an energy-efficient low-latency 3D-CNN accelerator is proposed. Temporal locality and small differential value dropout are used to increase the sparsity of activation. Furthermore, to fully utilize the sparsity of weight and activation, a full zero-skipping convolutional microarchitecture is proposed. A hierarchical load-balancing scheme is also introduced to improve resource utilization. With the proposed techniques, a 3D-CNN accelerator is designed in a 55-nm low-power CMOS technology, bringing in up to 9.89x speedup compared to the baseline implementation. Benchmarked with C3D, the proposed accelerator achieves an energy efficiency of 4.66 TOPS/W at 100 MHz and 1.08 V supply voltage.

</details>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIM 2025</div><img src='images/TIM25_2StageEEG.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A Two-Stage Prediction + Detection Framework for Real-Time Epileptic Seizure Monitoring

S. Qiu, W. Wang, **<u>C. Zhou</u>**, X. Song, J. Yang, and H. Jiao

*IEEE Transactions on Instrumentation and Measurement  (**TIM**)*


<details>
<summary>Abstract</summary>
The monitoring of epilepsy patients in non-hospital environment is highly desirable, where ultra-low power wearable devices are essential in such a system. The state-of-the-art epileptic seizure detection algorithms targeting such devices cannot achieve high sensitivity, short detection latency, low false alarm rate (FAR), as well as lightweight computing simultaneously. In this paper, we propose a two-stage prediction + detection deep neural network model, PDNet, for real-time epileptic seizure monitoring. The proposed two-stage PDNet model consists of a lightweight seizure predictor and a high-precision seizure detector. Only when the first-stage seizure predictor forecasts an impending seizure, the second-stage seizure detector is activated to precisely and rapidly classify the seizure states, thereby significantly suppressing the amount of computations. A semi-supervised learning strategy is employed to enhance the decision boundary of the seizure predictor, which is used for EEG pre-processing instead of pure prediction. Soft labels are adopted to enable the seizure detector to precisely classify the seizure states. The proposed PDNet is evaluated using the CHB-MIT scalp EEG database. When running the proposed prediction and detection models together for seizure detection purpose, the PDNet achieves 99.0% sensitivity, 0.54/h FAR, and 3.45-second detection latency with 3.03M multiplyâ€“accumulate (MAC) operations, which are competitive compared to the state of the art in terms of sensitivity, detection latency, FAR, and computation complexity. Furthermore, the fine-grained information such as the occurrence process of seizures demonstrated by soft labels can help the caregivers or clinicians to come up with targeted healthcare and clinical treatments.

</details>

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Symp. VLSI 2025</div><img src='images/VLSI25_PANDA.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

PANDA: A 3.178 TOPS/W Reconfigurable Seizure Prediction ANd Detection Neural Network Accelerator for Epilepsy Monitoring

S. Qiu, X. Song, X. Song, **<u>C. Zhou</u>**, X. Song, J. Yang, W. Wang, Y. Yang, and H. Jiao

*IEEE Symposium on VLSI Technology and Circuits (**Symp. VLSI**)*


<details>
<summary>Abstract</summary>
PANDA, a reconfigurable seizure prediction and detection neural network accelerator, is presented. A lightweight twostage seizure monitoring framework with temporal neural
network splitting is proposed to be deployed on PANDA. Channel first-output stationary dataflow with zero activation skipping and weight cache with statistical information are employed for higher energy efficiency. A flexible instruction set is defined to make PANDA highly configurable. For seizure monitoring, PANDA achieves up to 99% sensitivity, 0.43/h false alarm rate (FAR), and 3.178 TOPS/W energy efficiency.

</details>

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TBioCAS 2025</div><img src='images/EEG_TBioCAS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Energy-Efficient Configurable 1-D CNN-Based Multi-Lead ECG Classification Coprocessor for Wearable Cardiac Monitoring Devices](https://ieeexplore.ieee.org/abstract/document/10844856)

C. Zhang, Z. Huang, **<u>C. Zhou</u>**, A. Qie, and X. Wang

*IEEE Transactions on Biomedical Circuits and Systems (**TBioCAS**)*


<details>
<summary>Abstract</summary>
Many electrocardiogram (ECG) processors have been widely used for cardiac monitoring. However, most of them have relatively low energy efficiency, and lack configurability in classification leads number and inference algorithm models. A multi-lead ECG coprocessor is proposed in this paper, which can perform efficient ECG anomaly detection. In order to achieve high sensitivity and positive precision of R-peak detection, a method based on zero-crossing slope adaptive threshold comparison is proposed. Also, a one-dimensional convolutional neural network (1-D CNN) based classification engine with reconfigurable processing elements (PEs) is designed, good energy efficiency is achieved by combining filter level parallelism and output channel parallelism within the PE chains with register level data reuse strategy. To improve configurability, a single instruction multiple data (SIMD) based central controller is adopted, which facilitates ECG classification with configurable number of leads and updatable inference models. The proposed ECG coprocessor is fabricated using 55 nm CMOS technology, supporting classification with an accuracy of over 98%. The test results indicate that the chip consumes 62.2 nJ at 100 MHz, which is lower than most recent works. The energy efficiency reaches 397.1 GOPS/W, achieving an improvement of over 40% compared to the reported ECG processors using CNN models. The comparison results show that this design has advantages in energy overhead and configurability.

</details>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISCAS 2024</div><img src='images/ISCAS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Energy-Efficient Configurable Coprocessor Based on 1-D CNN for ECG Anomaly Detection](https://ieeexplore.ieee.org/abstract/document/10557838)

C. Zhang, Z. Huang, Q. Cheng, **<u>C. Zhou</u>**, and X. Wang

*IEEE International Symposium on Circuits and Systems (**ISCAS**)*


<details>
<summary>Abstract</summary>
Many healthcare devices have been widely used for electrocardiogram (ECG) monitoring. However, most of them have relatively low energy efficiency and lack flexibility. A novel ECG coprocessor is proposed in this paper, which can perform efficient ECG nomaly detection. In order to achieve high sensitivity and positive precision of R-peak detection, an algorithm based on Hilbert transform and adaptive threshold comparison is proposed. Also, a flexible one-dimensional convolutional neural network (1-D CNN) based classification engine is adopted, which can be configured with instructions to process various network models for ifferent applications. Good energy efficiency is achieved by combining filter level parallelism and output channel parallelism within the processing element (PE) array with data reuse strategy. A 1-D CNN for arrhythmia detection is proposed to validate the hardware performance. The proposed ECG coprocessor is implemented using 55 nm CMOS technology, occupying an area of 1.39 mm2. At a clock frequency of 100MHz, the energy efficiency is 215.6 nJ/classification. The comparison results show that this design has advantages in energy overhead and detection performance.

</details>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT 2024</div><img src='images/TCAS-I.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SoftAct: A High-Precision Softmax Architecture for Transformers with Nonlinear Functions Support](https://ieeexplore.ieee.org/document/10495359/)

Y. Fu, **<u>C. Zhou</u>**, T. Huang, E. Han, Y. He, and H. Jiao.

*IEEE Transactions on Circuits and Systems for Video Technology(**TCSVT**)*


<details>
<summary>Abstract</summary>
Transformer-based deep learning networks are revolutionizing our society. The convolution and attention codesigned (CAC) Transformers have demonstrated superior performance compared to the conventional Transformer-based networks. However, CAC Transformer networks contain various nonlinear functions, such as softmax and complex activation functions, which require high precision hardware design yet typically with significant cost in area and power consumption. To address these challenges, SoftAct, a compact and high-precision algorithm-hardware co-designed architecture, is proposed to implement both softmax and nonlinear activation functions in CAC Transformer accelerators. An improved softmax algorithm with penalties is proposed to maintain precision in hardware. A stage-wise full zero detection method is developed to skip redundant computation in softmax. A compact and reconfigurable architecture with a symmetrically designed linear fitting module is proposed to achieve nonlinear functions. The SoftAct architecture is designed in an industrial 28-nm CMOS technology with the MobileViT-xxs network as the benchmark. Compared with the state of the art, SoftAct achieves up to 35.14% network accuracy improvement, 10Ã— maximum frequency, and 809Ã— overall efficiency.

</details>

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT 2023</div><img src='images/TCSVT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CNN Accelerator at the Edge with Adaptive Zero Skipping and Sparsity-Driven Data Flow](https://ieeexplore.ieee.org/abstract/document/10122694/)

M. Liu, **<u>C. Zhou</u>**, S. Qiu, Y. He, and H. Jiao.

*IEEE Transactions on Circuits and Systems for Video Technology(**TCSVT**)*


<details>
<summary>Abstract</summary>
An energy-efficient convolutional neural network (CNN) accelerator is proposed for low-power inference on edge devices. An adaptive zero skipping technique is proposed to dynamically skip the zeros in either activations or weights, depending on which has the higher sparsity. The characteristic of non-zero data aggregation is explored to enhance the effectiveness of adaptive zero skipping in performance boosting. To mitigate the load imbalance issue after zero skipping, a sparsity-driven data flow and low-complexity dynamic task allocation are employed for different convolution layers. Facilitated further by a two-stage distiller, the proposed accelerator achieves 5.42Ã—, 3.41Ã—, and 3.42Ã— performance boosting for VGG16, AlexNet, and Mobilenet-v1, respectively, compared to the baseline. Implemented in a 55-nm low power CMOS technology, the proposed accelerator achieves an effective energy efficiency of 2.41 TOPS/W, 2.35 TOPS/W, and 0.64 TOPS/W for VGG16, AlexNet, and Mobilenet-v1, respectively, at 100 MHz and 1.08 V supply voltage.

</details>

</div>
</div>


# ğŸ€ Tape Out

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">GenAI</div><img src='images/DiffusionChip.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A energy-efficient diffusion chip for real-time and high-resolution images/videos generation at the edge.

12/2025 (Expected), Project Leader

*Fabricated in TSMC 16-nm FinFET technology with an expected area of 2 mmÃ—4 mm*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">HLS Chip</div><img src='images/HLSChip.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A high-level synthesis based chip for CNN/Transformer architectures in general CV/NLP applications.

06/2025 (Expected), Collaboration

*Fabricated in TSMC 16-nm FinFET technology with an expected area of 1 mmÃ—1 mm*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Transformer</div><img src='images/bit_variable_imc.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A bit-variable in-memory computing chip for accelerating transformers at the edge.

06/2025 (Expected), Collaboration

*Fabricated in TSMC 40-nm technology with an expected area of 1 mmÃ—1 mm*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Transformer</div><img src='images/nebula_v2_chip.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

An energy-efficient acceleration chip supporting transformer-based networks.

04/2025 (Expected), Project Leader

*Fabricated in TSMC 28-nm HPC technology with an expected area of 2 mmÃ—3 mm*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Point Cloud</div><img src='images/28nm.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

An energy-efficient pipelined and configurable 3D point cloud-based neural network accelerator.

08/2023, Project Leader

*Fabricated in TSMC 28-nm HPC technology with an area of 2.0 mmÃ—1.5 mm*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">3D-CNN</div><img src='images/55nm_micrograph.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A 4.5 TOPS/W sparse 3D-CNN accelerator for real-time 3D understanding

08/2020, Project Leader

*Fabricated in UMC 55-nm low-power CMOS technology with an area of 4.2 mmÃ—3.6 mm*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">1D-CNN</div><img src='images/1DCNN_Chips.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

1D-CNN accelerators for medical analysis

06/2024, Collaboration

*Fabricated in UMC 55-nm and TSMC 65-nm low-power CMOS technology*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2D-CNN</div><img src='images/LIU_28_Chip.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A 2.4 TOPS/W CNN accelerator with adaptive zero skipping and sparsity-driven dataflow

06/2022, Collaboration

*Fabricated in TSMC 28-nm HPC technology with an area of 2.0 mmÃ—1.35 mm*

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2D-CNN</div><img src='images/LIU_55_Chip.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

A 2.0 TOPS/W CNN accelerator skipping invalid activations

10/2019, Collaboration

*Fabricated in UMC 55-nm low-power CMOS technology with an area of 3.4 mmÃ—2.3 mm*

</div>
</div>




# ğŸ’» Skills
- Flow: IC Front-End, Logic Synthesis, FPGA, Neural Network Training
- Tools: Cadence, Vivado, PyTorch, TensorFlow 
- Language: Verilog, SystemVerilog, Python, C, Shell, Makefile

# ğŸ’¬ About Me
- I am an Easy Going and Self-Motivated Person. Feel Free to Reach out Anytime!
- Interests and Hobbies: Fitness, Taekwondo(Black Belt), and Table Tennis.


<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 


# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->
